{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53e123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nic/local-code/dsci-789/dsci-89/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/home/nic/local-code/dsci-789/dsci-89/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"scikit-learn>=1.7.2\" \\\n",
    "             \"numpy<=2.2\"\n",
    "\n",
    "%pip install \"ipykernel>=6.30.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed9008",
   "metadata": {},
   "source": [
    "First we are training a complex modelon the California housing data and evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fb97fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.514, R²: 0.798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Define and Train model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f}, R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dd54d",
   "metadata": {},
   "source": [
    "Next, we sample an instance from the test data with some constraints. The constraints are arbitrary; they are just so we can select a instance in a specific range that we are interested in. This is the instance we will explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2912f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "95th percentile of y_test: 4.764\n",
      "Random Instance:\n",
      "  Index: 5419\n",
      "  True value: 5.000\n",
      "  Predicted value: 3.894\n",
      "  Features\n",
      "   MedInc:3.9091\n",
      "   HouseAge:38.0\n",
      "   AveRooms:5.9021739130434785\n",
      "   AveBedrms:1.1875\n",
      "   Population:830.0\n",
      "   AveOccup:2.255434782608696\n",
      "   Latitude:34.02\n",
      "   Longitude:-118.43\n"
     ]
    }
   ],
   "source": [
    "def get_random_instance(y, X_sub, model, percentile=0.95, \n",
    "                        above=True, random_state=0):\n",
    "    \n",
    "    # Calculate value at given percentile\n",
    "    p = y.quantile(percentile)\n",
    "    # Select data from above or below the percentile threshold\n",
    "    subset = y[y >= p] if above else y[y <= p]\n",
    "    # Randomly sample from the subet\n",
    "    random_instance = subset.sample(n=1, random_state=random_state)\n",
    "    \n",
    "    # Extract the index and features of the chosen sample\n",
    "    idx = random_instance.index[0]\n",
    "    x_instance = X_sub.loc[[idx]]\n",
    "    \n",
    "    # Extract true and predicted values of the sample\n",
    "    true_val = random_instance.values[0]\n",
    "    pred_val = model.predict(x_instance)[0]\n",
    "    \n",
    "    # Zip features with labels for easy identificaiton later\n",
    "    features = zip(x_instance.columns, x_instance.values[0])\n",
    "\n",
    "    return {\n",
    "        \"idx\": idx,\n",
    "        \"true_val\": true_val,\n",
    "        \"pred_val\": pred_val,\n",
    "        \"percentile_value\": p,\n",
    "        \"features\": features,\n",
    "        \"x\": x_instance\n",
    "    }\n",
    "    \n",
    "def show_random_instance(instance_dict, percentile):\n",
    "    print(f\"\\n{percentile}th percentile of y_test: {instance_dict['percentile_value']:.3f}\")\n",
    "    print(f\"Random Instance:\")\n",
    "    print(f\"  Index: {instance_dict['idx']}\")\n",
    "    print(f\"  True value: {instance_dict['true_val']:.3f}\")\n",
    "    print(f\"  Predicted value: {instance_dict['pred_val']:.3f}\")\n",
    "    print(f\"  Features\")\n",
    "    for feature in instance_dict[\"features\"]:\n",
    "        print(f\"   {feature[0]}:{feature[1]}\")\n",
    "   \n",
    "\n",
    "local_instance = get_random_instance(y_test, X_test, model, \n",
    "                                     percentile=0.95, above=True)\n",
    "show_random_instance(local_instance, percentile=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcee487",
   "metadata": {},
   "source": [
    "Now, we follow a manual process to approximate the LIME approach. First, we generate a neighborhood of synthetic points around the sample. \n",
    "I have chosen to use a KNN approach to this where we:\n",
    "\n",
    "- Find the 50 nearest REAL data points from the training data using a KNN model\n",
    "- Sample them with replacement 1000 times\n",
    "- Take that set and perturb them slightly with noise so they are similar to the 50 real data points but not the same.\n",
    "\n",
    "This will hopefully help keep the features more realistic than pure noise based approach, since we will not accidentally create a possible but implausible points like houses with 10 rooms but only 1 bedroom or houses in areas with high population but very low occupancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578973e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def sample_neighbors(x, X_train, num_samples=1000, n_neighbors=50,\n",
    "                         jitter=True, jitter_scale=0.01):\n",
    "\n",
    "\n",
    "    # Fit a nearest neighbor model to the training data\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors).fit(X_train.to_numpy())\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    _, indices = nn.kneighbors(x)\n",
    "    candidate_neighbors = X_train.iloc[indices[0]]\n",
    "\n",
    "    # Sample those neighbors with replacement \n",
    "    sampled = candidate_neighbors.sample(n=num_samples, replace=True,\n",
    "                                         random_state=0).to_numpy()\n",
    "\n",
    "    # The points will overlap so we can add some noise to them\n",
    "    if jitter:\n",
    "        std = X_train.std(axis=0)\n",
    "        # The jitter_scale helps control how far we're moving the point \n",
    "        # its only 1% of the STD based on my defaults\n",
    "        noise = np.random.normal(scale=std * jitter_scale, size=sampled.shape)\n",
    "        sampled = sampled + noise\n",
    "\n",
    "        # Clip so samples stay in valid range\n",
    "        X_min, X_max = X_train.min(axis=0).to_numpy(), X_train.max(axis=0).to_numpy()\n",
    "        sampled = np.clip(sampled, X_min, X_max)\n",
    "\n",
    "    return sampled\n",
    "\n",
    "\n",
    "perturbed_data = sample_neighbors(local_instance[\"x\"], X_train)\n",
    "\n",
    "print(perturbed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d70a22f",
   "metadata": {},
   "source": [
    "Next we calculate the distance of each of the perturbed samples from the sample we want to explain. I am using Euclidian distance.\n",
    "\n",
    "We cannot use the distances directly because further data points have higher values which is the opposite of what we want. The LIME paper specified a gaussion kernel\n",
    "\n",
    "$$K(x, z) = \\exp(-\\frac{||x-z||^2}{\\sigma^2})$$\n",
    "\n",
    "I am using the default for the kernel width :  $\\sigma = 0.75 \\times \\sqrt{\\text{}num features}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2345a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_dist(point1, point2):\n",
    "    difference = point2 - point1\n",
    "    return np.linalg.norm(difference)\n",
    "\n",
    "distances = []\n",
    "for x in perturbed_data:\n",
    "    distances.append(euclidian_dist(local_instance[\"x\"], x))\n",
    "\n",
    "\n",
    "kernel_width = 0.75 * np.sqrt(perturbed_data.shape[1])\n",
    "weights = np.exp(-(np.array(distances) ** 2) / (kernel_width ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a6400",
   "metadata": {},
   "source": [
    "Finally, we use the original model to predict values for the perturbed data. \n",
    "\n",
    "We then fit a new linear model using the perturbed data, the predictions from the original black-box model and the weights.\n",
    "\n",
    "The coefficients of this model are our explanation weights. I have zipped them with their feature names, sorted them by abosolute value and printed them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51aa712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AveBedrms: \t4.1582\n",
      "AveOccup: \t-0.6961\n",
      "MedInc: \t0.4718\n",
      "Latitude: \t-0.1577\n",
      "Longitude: \t0.1315\n",
      "HouseAge: \t0.1152\n",
      "AveRooms: \t-0.0357\n",
      "Population: \t0.0056\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\") # supressing this warning because it doesn't amtter for what we're doing\n",
    "\n",
    "neighbor_preds = model.predict(perturbed_data)\n",
    "    \n",
    "local_model = LinearRegression()\n",
    "local_model.fit(perturbed_data, neighbor_preds, sample_weight=weights)\n",
    "explanation = dict(zip(X_train.columns, local_model.coef_))\n",
    "\n",
    "explanation = dict(sorted(explanation.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "\n",
    "for feat, weight in explanation.items():\n",
    "    print(f\"{feat}: \\t{weight:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci-89",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
